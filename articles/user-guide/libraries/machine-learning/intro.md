---
title: Bibliothèque de Machine Learning quantique
description: En savoir plus sur l’utilisation de Machine Learning sur des systèmes quantiques
author: alexeib2
ms.author: alexeib
ms.date: 11/22/2019
ms.topic: conceptual
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: e2f4a4a63eef40474856426b3b29652b5d3053b2
ms.sourcegitcommit: 71605ea9cc630e84e7ef29027e1f0ea06299747e
ms.translationtype: MT
ms.contentlocale: fr-FR
ms.lasthandoff: 01/26/2021
ms.locfileid: "98854035"
---
# <a name="introduction-to-quantum-machine-learning"></a><span data-ttu-id="6e16d-103">Présentation des Machine Learning Quantum</span><span class="sxs-lookup"><span data-stu-id="6e16d-103">Introduction to Quantum Machine Learning</span></span>

## <a name="framework-and-goals"></a><span data-ttu-id="6e16d-104">Infrastructure et objectifs</span><span class="sxs-lookup"><span data-stu-id="6e16d-104">Framework and goals</span></span>

<span data-ttu-id="6e16d-105">L’encodage et le traitement des informations Quantum constituent une alternative puissante aux classifieurs Quantum Machine Learning classiques.</span><span class="sxs-lookup"><span data-stu-id="6e16d-105">Quantum encoding and processing of information is a powerful alternative to classical machine learning Quantum classifiers.</span></span> <span data-ttu-id="6e16d-106">En particulier, il nous permet d’encoder des données dans des registres quantiques concis par rapport au nombre de fonctionnalités, en employant systématiquement l’enchevêtrement quantique comme ressource de calcul et en employant une mesure de Quantum pour l’inférence de classe.</span><span class="sxs-lookup"><span data-stu-id="6e16d-106">In particular, it allows us to encode data in quantum registers that are concise relative to the number of features, systematically employing quantum entanglement as computational resource and employing quantum measurement for class inference.</span></span>
<span data-ttu-id="6e16d-107">Le classifieur Quantum axé sur les circuits est une solution Quantum relativement simple qui associe l’encodage des données à un circuit Quantum démêler la rapide suivi d’une mesure pour déduire les étiquettes de classe des exemples de données.</span><span class="sxs-lookup"><span data-stu-id="6e16d-107">Circuit centric quantum classifier is a relatively simple quantum solution that combines data encoding with a rapidly entangling/disentangling quantum circuit followed by measurement to infer class labels of data samples.</span></span>
<span data-ttu-id="6e16d-108">L’objectif est de garantir la caractérisation et le stockage classiques des circuits de sujet, ainsi que la formation hybride Quantum/classique des paramètres de circuit, même pour les espaces de fonctionnalités extrêmement volumineux.</span><span class="sxs-lookup"><span data-stu-id="6e16d-108">The goal is to ensure classical characterization and storage of subject circuits, as well as hybrid quantum/classical training of the circuit parameters even for extremely large feature spaces.</span></span>

## <a name="classifier-architecture"></a><span data-ttu-id="6e16d-109">Architecture du classifieur</span><span class="sxs-lookup"><span data-stu-id="6e16d-109">Classifier architecture</span></span>

<span data-ttu-id="6e16d-110">La classification est une tâche de Machine Learning supervisée, où l’objectif est de déduire les étiquettes de classe $ \{ Y_1, y_2, \ldots, y_d \} $ de certains échantillons de données.</span><span class="sxs-lookup"><span data-stu-id="6e16d-110">Classification is a supervised machine learning task, where the goal is to infer class labels $\{y_1,y_2,\ldots,y_d\}$ of certain data samples.</span></span> <span data-ttu-id="6e16d-111">Le « jeu de données d’apprentissage » est une collection d’échantillons $ \mathcal{D} = \{ (x, y)} $ avec des étiquettes préaffectées connues.</span><span class="sxs-lookup"><span data-stu-id="6e16d-111">The "training data set" is a collection of samples $\mathcal{D}=\{(x,y)}$ with known pre-assigned labels.</span></span> <span data-ttu-id="6e16d-112">Ici $x $ est un exemple de données et $y $ est son étiquette connue appelée « étiquette de formation ».</span><span class="sxs-lookup"><span data-stu-id="6e16d-112">Here $x$ is a data sample and $y$ is its known label called "training label".</span></span>
<span data-ttu-id="6e16d-113">À l’instar des méthodes traditionnelles, la classification Quantum se compose de trois étapes :</span><span class="sxs-lookup"><span data-stu-id="6e16d-113">Somewhat similar to traditional methods, quantum classification consists of three steps:</span></span>
- <span data-ttu-id="6e16d-114">encodage des données</span><span class="sxs-lookup"><span data-stu-id="6e16d-114">data encoding</span></span>
- <span data-ttu-id="6e16d-115">préparation d’un état de classifieur</span><span class="sxs-lookup"><span data-stu-id="6e16d-115">preparation of a classifier state</span></span>
- <span data-ttu-id="6e16d-116">mesure en raison de la nature probabiliste de la mesure, ces trois étapes doivent être répétées plusieurs fois.</span><span class="sxs-lookup"><span data-stu-id="6e16d-116">measurement Due to the probabilistic nature of the measurement, these three steps must be repeated multiple times.</span></span> <span data-ttu-id="6e16d-117">L’encodage et le calcul de l’état du classifieur sont effectués au moyen de *circuits quantiques*.</span><span class="sxs-lookup"><span data-stu-id="6e16d-117">Both the encoding and the computing of the classifier state are done by means of *quantum circuits*.</span></span> <span data-ttu-id="6e16d-118">Alors que le circuit d’encodage est généralement piloté par les données et sans paramètre, le circuit de classifieur contient un ensemble suffisant de paramètres en cours d’apprentissage.</span><span class="sxs-lookup"><span data-stu-id="6e16d-118">While the encoding circuit is usually data-driven and parameter-free, the classifier circuit contains a sufficient set of learnable parameters.</span></span> 

<span data-ttu-id="6e16d-119">Dans la solution proposée, le circuit de classifieur est composé de rotations à qubit unique et de rotations contrôlées à deux qubit.</span><span class="sxs-lookup"><span data-stu-id="6e16d-119">In the proposed solution the classifier circuit is composed of single-qubit rotations and two-qubit controlled rotations.</span></span> <span data-ttu-id="6e16d-120">Les paramètres en apprentissage ici sont les angles de rotation.</span><span class="sxs-lookup"><span data-stu-id="6e16d-120">The learnable parameters here are the rotation angles.</span></span> <span data-ttu-id="6e16d-121">Les portes de rotation et de rotation contrôlée sont connues pour être *universelles* pour le calcul Quantum, ce qui signifie que toute matrice de poids unitaire peut être décomposée en un circuit suffisamment long constitué de ces portes.</span><span class="sxs-lookup"><span data-stu-id="6e16d-121">The rotation and controlled rotation gates are known to be *universal* for quantum computation, which means that any unitary weight matrix can be decomposed into a long enough circuit consisting of such gates.</span></span>

<span data-ttu-id="6e16d-122">Dans la version proposée, un seul circuit suivi d’une seule estimation de fréquence est pris en charge.</span><span class="sxs-lookup"><span data-stu-id="6e16d-122">In the proposed version, only one circuit followed by a single frequency estimation is supported.</span></span>
<span data-ttu-id="6e16d-123">Par conséquent, la solution est une analogie quantique d’une machine à vecteurs de support avec un noyau polynomial à faible degré.</span><span class="sxs-lookup"><span data-stu-id="6e16d-123">Thus, the solution is a quantum analog of a support vector machine with a low-degree polynomial kernel.</span></span>

![Perceptron multicouches et classificateur centré sur les circuits](~/media/DLvsQCC.png)

<span data-ttu-id="6e16d-125">Une conception de classifieur Quantum simple peut être comparée à une solution SVM (support vector machine) classique.</span><span class="sxs-lookup"><span data-stu-id="6e16d-125">A simple quantum classifier design can be compared to a traditional support vector machine (SVM) solution.</span></span> <span data-ttu-id="6e16d-126">L’inférence pour un exemple de données $x $ dans le cas de SVM est effectuée à l’aide d’une forme de noyau optimale $ \sum \ alpha_j k (x_j, x) $ où $k $ est une fonction de noyau spécifique.</span><span class="sxs-lookup"><span data-stu-id="6e16d-126">The inference for a data sample $x$ in case of SVM is done using an optimal kernel form $\sum \alpha_j  k(x_j,x)$ where $k$ is a certain kernel function.</span></span>

<span data-ttu-id="6e16d-127">En revanche, un classifieur quantum utilise le $p de prédiction (o x, U (\Theta)) = 〈 U (\Theta) x | M | U (\Theta) x 〉 $, qui est similaire à l’esprit, mais techniquement très différent.</span><span class="sxs-lookup"><span data-stu-id="6e16d-127">By contrast, a quantum classifier uses the predictor $p(y│x,U(\theta))=〈U(\theta)x|M|U(\theta)x〉$, which is similar in spirit but technically quite different.</span></span> <span data-ttu-id="6e16d-128">Ainsi, lorsqu’un encodage d’amplitude simple est utilisé, $p (o x, U (\Theta)) $ est une forme quadratique dans les amplitudes de $x $, mais les coefficients de ce formulaire ne sont plus appris de manière indépendante. ils sont à la place regroupés à partir des éléments de matrice du circuit $U (\Theta) $, qui a généralement beaucoup moins de paramètres d’apprentissage $ \Theta $ que la dimension du vecteur $x $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-128">Thus, when a straightforward amplitude encoding is used,  $p(y│x,U(\theta))$ is a quadratic form in the amplitudes of $x$, but the coefficients of this form are no longer learned independently; they are instead aggregated from the matrix elements of the circuit $U(\theta)$, which typically has significantly fewer learnable parameters $\theta$ than the dimension of the vector $x$.</span></span> <span data-ttu-id="6e16d-129">Le degré polynomial de $p (y x │ x, U (\Theta)) $ dans les fonctionnalités d’origine peut être augmenté jusqu’à $2 ^ l $ en utilisant un encodage de produit Quantum sur $l $ copies de $x $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-129">The polynomial degree of $p(y│x,U(\theta))$ in the original features can be increased to $2^l$ by using a quantum product encoding on $l$ copies of $x$.</span></span>

<span data-ttu-id="6e16d-130">Notre architecture explore des circuits relativement superficiels, qui doivent donc être *rapidement emmêlants* afin de capturer toutes les corrélations entre les fonctionnalités de données de toutes les plages.</span><span class="sxs-lookup"><span data-stu-id="6e16d-130">Our architecture explores relatively shallow circuits, which therefore must be *rapidly entangling* in order to capture all the correlations between the data features at all ranges.</span></span> <span data-ttu-id="6e16d-131">La figure ci-dessous montre un exemple du composant de circuit le plus rapide et le plus utile.</span><span class="sxs-lookup"><span data-stu-id="6e16d-131">An example of the most useful rapidly entangling circuit component is shown on figure below.</span></span> <span data-ttu-id="6e16d-132">Même si un circuit avec cette géométrie se compose uniquement de $3 n + 1 $ portes, la matrice de poids unitaire qu’il calcule assure une communication croisée importante entre les fonctionnalités de $2 ^ n $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-132">Even though a circuit with this geometry consists of only $3 n+1$ gates, the unitary weight matrix that it computes ensures significant cross-talk between $2^n$ features.</span></span>

![Circuit quantique de 5 qubits (avec deux couches cycliques) rapidement emmêlant le circuit Quantum.](~/media/5-qubit-qccc.png)

<span data-ttu-id="6e16d-134">Le circuit dans l’exemple ci-dessus est constitué de 6 G_1 portes qubit, \ldots, G_5 ; G_ {16} ) $ et 10 2-qubits Gates $ (G_6, \ldots, g_ {15} ) $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-134">The circuit in the above example consists of 6 single-qubit gates $(G_1,\ldots,G_5; G_{16})$ and 10 two-qubits gates $(G_6,\ldots,G_{15})$.</span></span> <span data-ttu-id="6e16d-135">En supposant que chacune des portes est définie avec un paramètre en cours d’apprentissage, nous avons 16 paramètres d’apprentissage, tandis que la dimension de l’espace de 5 qubit Hilbert est de 32.</span><span class="sxs-lookup"><span data-stu-id="6e16d-135">Assuming that each of the gates is defined with one learnable parameter we have 16 learnable parameters, while the dimension of the 5-qubit Hilbert space is 32.</span></span> <span data-ttu-id="6e16d-136">Une telle géométrie de circuit peut être facilement généralisée à tout $n Registre $-qubit, lorsque $n $ est impair, produisant des circuits avec $3 n + 1 $ paramètres pour l’espace de fonctionnalité de $2 ^ n $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-136">Such circuit geometry can be easily generalized to any $n$-qubit register, when $n$ is odd, yielding circuits with $3 n+1$ parameters for $2^n$-dimensional feature space.</span></span>

## <a name="classifier-training-as-a-supervised-learning-task"></a><span data-ttu-id="6e16d-137">Apprentissage du classifieur en tant que tâche d’apprentissage supervisé</span><span class="sxs-lookup"><span data-stu-id="6e16d-137">Classifier training as a supervised learning task</span></span>

<span data-ttu-id="6e16d-138">La formation d’un modèle de classifieur implique la recherche de valeurs optimales de ses paramètres opérationnels, de sorte qu’ils optimisent la probabilité moyenne d’inférence des étiquettes d’apprentissage appropriées dans les exemples d’apprentissage.</span><span class="sxs-lookup"><span data-stu-id="6e16d-138">Training of a classifier model involves finding optimal values of its operational parameters, such that they maximize the average likelihood of inferring the correct training labels across the training samples.</span></span>
<span data-ttu-id="6e16d-139">Ici, nous nous soucions de la classification à deux niveaux uniquement, c’est-à-dire le cas de $d = $2 et seulement deux classes avec les étiquettes $y _ 1, y_2 $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-139">Here, we concern ourselves with two level classification only, i.e. the case of $d=2$ and only two classes with the labels $y_1,y_2$.</span></span>

> [!NOTE]
> <span data-ttu-id="6e16d-140">La généralisation de la généralisation de nos méthodes à un nombre arbitraire de classes consiste à remplacer qubits par qudits, c.-à-d. les unités quantiques avec des États de base $d $ et la mesure bidirectionnelle avec $d mesure en $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-140">A principled way of generalizing our methods to arbitrary number of classes is to replace qubits with qudits, i.e. quantum units with $d$ basis states, and the two-way measurement with $d$-way measurement.</span></span>

### <a name="likelihood-as-the-training-goal"></a><span data-ttu-id="6e16d-141">Probabilité de l’objectif de formation</span><span class="sxs-lookup"><span data-stu-id="6e16d-141">Likelihood as the training goal</span></span>

<span data-ttu-id="6e16d-142">Dans le cas d’un $U circuit Quantum \Theta () $, où $ \Theta $ est un vecteur de paramètres et qui dénote la mesure finale par $M $, la probabilité moyenne de l’inférence de l’étiquette correcte est $ $ \begin{align} \mathcal{L} (\Theta) = \frac {1} {| \mathcal{D} |} \left (\ sum_ {(x, Y_1) \In\mathcal{D}} P (M = Y_1 | U (\Theta) x) + \ sum_ {(x, y_2) \in\mathcal{D}} P (M = y_2 | U (\Theta) x) \right) \end{align} $ $ où $P (M = y | z) $ est la probabilité de mesurer $y $ dans l’État Quantum $z $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-142">Given a learnable quantum circuit $U(\theta)$, where $\theta$ is a vector of parameters, and denoting the final measurement by $M$, the average likelihood of the correct label inference is $$ \begin{align} \mathcal{L}(\theta)=\frac{1}{|\mathcal{D}|} \left( \sum_{(x,y_1)\in\mathcal{D}} P(M=y_1|U(\theta) x) + \sum_{(x,y_2)\in\mathcal{D}} P(M=y_2|U(\theta) x)\right) \end{align} $$ where $P(M=y|z)$ is the probability of measuring $y$ in quantum state $z$.</span></span>
<span data-ttu-id="6e16d-143">Ici, il suffit de comprendre que la fonction de probabilité $ \mathcal{L} (\Theta) $ est lisse dans $ \Theta $ et que son dérivé dans un $ \ theta_j $ peut être calculé en faisant essentiellement le même protocole Quantum que celui utilisé pour calculer la fonction de probabilité proprement dite.</span><span class="sxs-lookup"><span data-stu-id="6e16d-143">Here, it suffices to understand that the likelihood function $\mathcal{L}(\theta)$ is smooth in $\theta$ and its derivative in any $\theta_j$ can be computed by essentially the same quantum protocol as used for computing the likelihood function itself.</span></span> <span data-ttu-id="6e16d-144">Cela permet d’optimiser la profondeur du gradient $ \mathcal{L} (\Theta) $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-144">This allows for optimizing the $\mathcal{L}(\theta)$ by gradient descent.</span></span>

### <a name="classifier-bias-and-training-score"></a><span data-ttu-id="6e16d-145">Décalage du classifieur et score de formation</span><span class="sxs-lookup"><span data-stu-id="6e16d-145">Classifier bias and training score</span></span>

<span data-ttu-id="6e16d-146">Compte tenu de certaines valeurs intermédiaires (ou finales) des paramètres de $ \Theta $, nous devons identifier une seule valeur réelle $b $ savent comme *biais de classifieur* pour effectuer l’inférence.</span><span class="sxs-lookup"><span data-stu-id="6e16d-146">Given some intermediate (or final) values of the parameters in $\theta$, we need to identify a single real value $b$ know as *classifier bias* to do the inference.</span></span> <span data-ttu-id="6e16d-147">La règle d’inférence d’étiquette fonctionne comme suit :</span><span class="sxs-lookup"><span data-stu-id="6e16d-147">The label inference rule works as follows:</span></span> 
- <span data-ttu-id="6e16d-148">Une étiquette $y _2 $ est assignée à un exemple $x $ si et seulement si $P (M = y_2 | U (\Theta) x) + b > $0,5 (RULE1) (dans le cas contraire, une étiquette $y 1 _ 1 $) est affectée</span><span class="sxs-lookup"><span data-stu-id="6e16d-148">A sample $x$ is assigned label $y_2$ if and only if $P(M=y_2|U(\theta) x) + b > 0.5$  (RULE1) (otherwise it is assigned label $y_1$)</span></span>

<span data-ttu-id="6e16d-149">Clairement $b $ doit être dans l’intervalle $ (-0.5, + 0,5) $ pour être significatif.</span><span class="sxs-lookup"><span data-stu-id="6e16d-149">Clearly $b$ must be in the interval $(-0.5,+0.5)$ to be meaningful.</span></span>

<span data-ttu-id="6e16d-150">Un cas d’apprentissage $ (x, y) \Dans \mathcal{D} $ est considéré comme une *classification* incorrecte en raison de l’écart $b $ si l’étiquette déduite pour $x $ comme par Rule1 est en fait différente de $y $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-150">A training case $(x,y) \in \mathcal{D}$ is considered a *misclassification* given the bias $b$ if the label inferred for $x$ as per RULE1 is actually different from $y$.</span></span> <span data-ttu-id="6e16d-151">Le nombre global de classifications incorrectes est le *score d’apprentissage* du classifieur en fonction de l’écart $b $.</span><span class="sxs-lookup"><span data-stu-id="6e16d-151">The overall number of misclassifications is the *training score* of the classifier given the bias $b$.</span></span> <span data-ttu-id="6e16d-152">L’écart *optimal* du classifieur $b $ minimise le score de formation.</span><span class="sxs-lookup"><span data-stu-id="6e16d-152">The *optimal* classifier bias $b$ minimizes the training score.</span></span> <span data-ttu-id="6e16d-153">Il est facile de le voir, étant donné les estimations de probabilité précalculées $ \{ P (M = y_2 | U (\Theta) x) | (x, \*) \in\mathcal{D} \} $, l’écart optimal du classifieur peut être trouvé par la recherche binaire dans Interval $ (-0.5, + 0.5) $ en effectuant au maximum $ \ Log_2 (| \mathcal{D} |) $ étapes.</span><span class="sxs-lookup"><span data-stu-id="6e16d-153">It is easy to see that, given the precomputed probability estimates $\{ P(M=y_2|U(\theta) x) | (x,\*)\in\mathcal{D} \}$, the optimal classifier bias can be found by binary search in interval $(-0.5,+0.5)$ by making at most $\log_2(|\mathcal{D}|)$ steps.</span></span>

### <a name="reference"></a><span data-ttu-id="6e16d-154">Référence</span><span class="sxs-lookup"><span data-stu-id="6e16d-154">Reference</span></span>

<span data-ttu-id="6e16d-155">Ces informations doivent être suffisantes pour commencer à utiliser le code.</span><span class="sxs-lookup"><span data-stu-id="6e16d-155">This information should be enough to start playing with the code.</span></span> <span data-ttu-id="6e16d-156">Toutefois, si vous souhaitez en savoir plus sur ce modèle, veuillez lire la proposition d’origine : [ *« classificateurs quantiques centrés sur le circuit », Maria Schuld, Alex Bocharov, Krysta Svore et Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span><span class="sxs-lookup"><span data-stu-id="6e16d-156">However, if you want to learn more about this model, please read the original proposal: [*'Circuit-centric quantum classifiers', Maria Schuld, Alex Bocharov, Krysta Svore and Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span></span>

<span data-ttu-id="6e16d-157">En plus de l’exemple de code que vous verrez dans les étapes suivantes, vous pouvez également commencer à explorer la classification quantique dans [ce didacticiel](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification) .</span><span class="sxs-lookup"><span data-stu-id="6e16d-157">In addition to the code sample you will see in the next steps, you can also start exploring quantum classification in [this tutorial](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)</span></span> 
